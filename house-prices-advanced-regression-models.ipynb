{"metadata":{"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"}},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import Lasso\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nfrom sklearn.metrics import mean_squared_log_error\nfrom sklearn.metrics import mean_squared_error\nfrom xgboost import XGBRegressor\nimport math","metadata":{"execution":{"iopub.status.busy":"2022-06-16T12:32:43.257523Z","iopub.execute_input":"2022-06-16T12:32:43.257968Z","iopub.status.idle":"2022-06-16T12:32:43.981548Z","shell.execute_reply.started":"2022-06-16T12:32:43.257934Z","shell.execute_reply":"2022-06-16T12:32:43.980498Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"import and load the datasets ","metadata":{}},{"cell_type":"code","source":"train_data = pd.read_csv('../input/house-prices-advanced-regression-techniques/train.csv')\ntest_data = pd.read_csv('../input/house-prices-advanced-regression-techniques/test.csv')","metadata":{"execution":{"iopub.status.busy":"2022-06-16T12:41:05.002773Z","iopub.execute_input":"2022-06-16T12:41:05.003594Z","iopub.status.idle":"2022-06-16T12:41:05.064657Z","shell.execute_reply.started":"2022-06-16T12:41:05.003550Z","shell.execute_reply":"2022-06-16T12:41:05.063702Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"train_data.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-16T12:41:41.662479Z","iopub.execute_input":"2022-06-16T12:41:41.662945Z","iopub.status.idle":"2022-06-16T12:41:41.696304Z","shell.execute_reply.started":"2022-06-16T12:41:41.662908Z","shell.execute_reply":"2022-06-16T12:41:41.695276Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"test_data.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-16T12:41:51.509355Z","iopub.execute_input":"2022-06-16T12:41:51.509918Z","iopub.status.idle":"2022-06-16T12:41:51.542217Z","shell.execute_reply.started":"2022-06-16T12:41:51.509873Z","shell.execute_reply":"2022-06-16T12:41:51.541073Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"train_data.describe()","metadata":{"execution":{"iopub.status.busy":"2022-06-16T12:41:58.808488Z","iopub.execute_input":"2022-06-16T12:41:58.809139Z","iopub.status.idle":"2022-06-16T12:41:58.943363Z","shell.execute_reply.started":"2022-06-16T12:41:58.809097Z","shell.execute_reply":"2022-06-16T12:41:58.941800Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"train_data.info()","metadata":{"execution":{"iopub.status.busy":"2022-06-16T12:42:08.076406Z","iopub.execute_input":"2022-06-16T12:42:08.076842Z","iopub.status.idle":"2022-06-16T12:42:08.104725Z","shell.execute_reply.started":"2022-06-16T12:42:08.076808Z","shell.execute_reply":"2022-06-16T12:42:08.103688Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"raw","source":"Lets start observing more, before that I want to make a copy of train data as it doesnt affect the original train data\n","metadata":{}},{"cell_type":"code","source":"all_features_except_target = [i for i in train_data.columns if i not in ['SalePrice'] ]\nfeatures_X = train_data[all_features_except_target]\ny = train_data['SalePrice']\n\n\ntrain_data_copy = features_X.copy()\ntrain_data_copy.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-16T12:42:29.992902Z","iopub.execute_input":"2022-06-16T12:42:29.993627Z","iopub.status.idle":"2022-06-16T12:42:30.023470Z","shell.execute_reply.started":"2022-06-16T12:42:29.993593Z","shell.execute_reply":"2022-06-16T12:42:30.022697Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"before starting any preprocessing, i would like to analyse more. Lets explore through visualization","metadata":{}},{"cell_type":"code","source":"sns.displot(train_data['SalePrice'])","metadata":{"execution":{"iopub.status.busy":"2022-06-16T12:42:42.846029Z","iopub.execute_input":"2022-06-16T12:42:42.846469Z","iopub.status.idle":"2022-06-16T12:42:43.252325Z","shell.execute_reply.started":"2022-06-16T12:42:42.846434Z","shell.execute_reply":"2022-06-16T12:42:43.251188Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"train_data['SalePrice'].describe()","metadata":{"execution":{"iopub.status.busy":"2022-06-16T12:42:51.226082Z","iopub.execute_input":"2022-06-16T12:42:51.226524Z","iopub.status.idle":"2022-06-16T12:42:51.240196Z","shell.execute_reply.started":"2022-06-16T12:42:51.226491Z","shell.execute_reply":"2022-06-16T12:42:51.239164Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"From the above plot and describe() function, we can conclude that the plot is left skewed and the plot is densed between 100k and 300k in X_axis.\n\nAs there are many features, it will be easy for us to plot a correlation map as it will help us to learn more about correlations between each variable with target and other variables.\n","metadata":{}},{"cell_type":"code","source":"corr_mat = train_data.corr()\nf, ax = plt.subplots(figsize=(12, 9))\nsns.heatmap(corr_mat, vmax=0.8, square=True)","metadata":{"execution":{"iopub.status.busy":"2022-06-16T12:42:59.034042Z","iopub.execute_input":"2022-06-16T12:42:59.034496Z","iopub.status.idle":"2022-06-16T12:43:00.050317Z","shell.execute_reply.started":"2022-06-16T12:42:59.034462Z","shell.execute_reply":"2022-06-16T12:43:00.049221Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"The plot above shows the relationship between each variable with other, there are negative, positive and no relationships. As we would like to know more positively correlated features, lets filter the map to display positively correlated features.","metadata":{}},{"cell_type":"code","source":"positive_correlations = train_data.corr()\npositive_feature_columns = positive_correlations['SalePrice'][positive_correlations['SalePrice'].values > 0.2].index.values\npositive_feature_columns","metadata":{"execution":{"iopub.status.busy":"2022-06-16T12:43:08.713123Z","iopub.execute_input":"2022-06-16T12:43:08.713545Z","iopub.status.idle":"2022-06-16T12:43:08.730160Z","shell.execute_reply.started":"2022-06-16T12:43:08.713511Z","shell.execute_reply":"2022-06-16T12:43:08.729142Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"positive_corr_mat = train_data[positive_feature_columns].corr()\nf, ax = plt.subplots(figsize=(16, 16))\nsns.heatmap(positive_corr_mat, vmax=0.8, square=True, annot=True)","metadata":{"execution":{"iopub.status.busy":"2022-06-16T12:43:14.353108Z","iopub.execute_input":"2022-06-16T12:43:14.353558Z","iopub.status.idle":"2022-06-16T12:43:17.485075Z","shell.execute_reply.started":"2022-06-16T12:43:14.353523Z","shell.execute_reply":"2022-06-16T12:43:17.483800Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"From the above heatmap, the following observations are made:\n\n* OverallQual', 'GrLivArea' , 'TotalBsmtSF','GarageCars' and 'GarageArea' have strong correlation with 'SalePrice'.\n* 'GarageCars' and 'GarageArea'are mutually dependent\n* Same mutual dependence applies to the two features 'TotalBsmtSF' and '1stFloor' . We will take only 'TotalBsmtSF' in our feature-engineering.\n* AND ALSO 'TotRmsAbvGrd' and 'GrLivArea', and we will only take 'GrLivArea'","metadata":{}},{"cell_type":"markdown","source":"Lets check out the missing data","metadata":{}},{"cell_type":"code","source":"total = train_data.isnull().sum().sort_values(ascending=False)\npercent = (train_data.isnull().sum() / train_data.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data.head(20)","metadata":{"execution":{"iopub.status.busy":"2022-06-16T12:43:25.286106Z","iopub.execute_input":"2022-06-16T12:43:25.286548Z","iopub.status.idle":"2022-06-16T12:43:25.326875Z","shell.execute_reply.started":"2022-06-16T12:43:25.286514Z","shell.execute_reply":"2022-06-16T12:43:25.325721Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"Lets now work on missing values in the columns. Lets first do the numerical and categorical values. ","metadata":{}},{"cell_type":"code","source":"#Before that while observing the positively correlated variables, i came across some missing values in 'GarageYrBlt' and \n#i feel that its ok to be replaced by the 'YearBuilt' as its better than replacing with 0.\ntrain_data_copy.loc[train_data.GarageYrBlt.isnull(),'GarageYrBlt'] = train_data_copy.loc[train_data.GarageYrBlt.isnull(),'YearBuilt']\n","metadata":{"execution":{"iopub.status.busy":"2022-06-16T12:43:35.998730Z","iopub.execute_input":"2022-06-16T12:43:35.999156Z","iopub.status.idle":"2022-06-16T12:43:36.008271Z","shell.execute_reply.started":"2022-06-16T12:43:35.999122Z","shell.execute_reply":"2022-06-16T12:43:36.006863Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"#Numerical columns\nfeature_numerical_columns = [col_name for col_name in train_data_copy.columns if\n                train_data_copy[col_name].dtype in ['int64', 'float64']]","metadata":{"execution":{"iopub.status.busy":"2022-06-16T12:43:40.934898Z","iopub.execute_input":"2022-06-16T12:43:40.935324Z","iopub.status.idle":"2022-06-16T12:43:40.945333Z","shell.execute_reply.started":"2022-06-16T12:43:40.935291Z","shell.execute_reply":"2022-06-16T12:43:40.944181Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"feature_categorical_cols = [col_name for col_name in train_data_copy.columns if\n                    train_data_copy[col_name].nunique() < 50 and\n                    train_data_copy[col_name].dtype in ['object', 'bool']]","metadata":{"execution":{"iopub.status.busy":"2022-06-16T12:43:47.896723Z","iopub.execute_input":"2022-06-16T12:43:47.897173Z","iopub.status.idle":"2022-06-16T12:43:47.916201Z","shell.execute_reply.started":"2022-06-16T12:43:47.897139Z","shell.execute_reply":"2022-06-16T12:43:47.915065Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"Lets replace the null values in numerical columns with mean or meridian using imputer. Before that lets modify some columns","metadata":{}},{"cell_type":"code","source":"train_data_copy['years_since_update'] = train_data_copy['YearRemodAdd'] - train_data_copy['YearBuilt']\ntrain_data_copy['garage_value'] = train_data_copy['YearBuilt'] * train_data_copy['GarageCars']\n\n#train_data_copy = train_data_copy.drop(columns=['GarageCars'])","metadata":{"execution":{"iopub.status.busy":"2022-06-16T12:45:06.246312Z","iopub.execute_input":"2022-06-16T12:45:06.246787Z","iopub.status.idle":"2022-06-16T12:45:06.255386Z","shell.execute_reply.started":"2022-06-16T12:45:06.246752Z","shell.execute_reply":"2022-06-16T12:45:06.254353Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"feature_numerical_transformer = SimpleImputer(strategy='mean')","metadata":{"execution":{"iopub.status.busy":"2022-06-16T12:45:10.962443Z","iopub.execute_input":"2022-06-16T12:45:10.962887Z","iopub.status.idle":"2022-06-16T12:45:10.968105Z","shell.execute_reply.started":"2022-06-16T12:45:10.962851Z","shell.execute_reply":"2022-06-16T12:45:10.966714Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"Lets replace NaN values with frequently used values using imputer","metadata":{}},{"cell_type":"code","source":"feature_categorical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n])","metadata":{"execution":{"iopub.status.busy":"2022-06-16T12:45:22.446257Z","iopub.execute_input":"2022-06-16T12:45:22.446850Z","iopub.status.idle":"2022-06-16T12:45:22.453597Z","shell.execute_reply.started":"2022-06-16T12:45:22.446796Z","shell.execute_reply":"2022-06-16T12:45:22.452183Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"feature_preprocessor = ColumnTransformer(\n    transformers=[\n        ('num', feature_numerical_transformer, feature_numerical_columns),\n        ('cat', feature_categorical_transformer, feature_categorical_cols),\n        \n])","metadata":{"execution":{"iopub.status.busy":"2022-06-16T13:46:31.567587Z","iopub.execute_input":"2022-06-16T13:46:31.568231Z","iopub.status.idle":"2022-06-16T13:46:31.573931Z","shell.execute_reply.started":"2022-06-16T13:46:31.568180Z","shell.execute_reply":"2022-06-16T13:46:31.572954Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"markdown","source":"Lets model our data ","metadata":{}},{"cell_type":"code","source":"#Linear Regression:\n#regr_model = LinearRegression()\nfeature_classifier = Pipeline(steps=[('feature_preprocessor', feature_preprocessor),\n                      ('feature_model',LinearRegression(fit_intercept=True)),\n                     ])\n\nfeature_X_train, feature_X_valid, feature_y_train, feature_y_test = train_test_split(train_data_copy, y, random_state=0)\n\nfeature_classifier.fit(feature_X_train, feature_y_train)\nfeature_predictions = feature_classifier.predict(feature_X_valid)\n\nprint('Root Mean Square Error is ', np.sqrt(mean_squared_error(feature_y_test, feature_predictions)))\n\n\n# Implementing Scikit-learn function for RMSLE\nprint('RMSLE using scikit-learn function', mean_squared_log_error(feature_y_test, feature_predictions))\n#print('Intercept is ', regr_model.intercept_)\n\n# For retrieving the slope (coefficient of x). This will be an array of values.\n#print(\"Slope i.e. coefficient of x is \", regr_model.coef_)\n","metadata":{"execution":{"iopub.status.busy":"2022-06-16T13:55:08.755939Z","iopub.execute_input":"2022-06-16T13:55:08.756486Z","iopub.status.idle":"2022-06-16T13:55:09.117615Z","shell.execute_reply.started":"2022-06-16T13:55:08.756432Z","shell.execute_reply":"2022-06-16T13:55:09.115965Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"feature_model = XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n             colsample_bynode=1, colsample_bytree=0.6, gamma=0.0, gpu_id=-1,\n             importance_type='gain', interaction_constraints='',\n             learning_rate=0.02, max_delta_step=0, max_depth=4,\n             min_child_weight=0.0, monotone_constraints='()',\n             n_estimators=1250, n_jobs=0, num_parallel_tree=1, random_state=0,\n             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=0.8,\n             tree_method='exact', validate_parameters=1, verbosity=None)\n\nfeature_classifier = Pipeline(steps=[('feature_preprocessor', feature_preprocessor),\n                      ('feature_model', feature_model)\n                     ])\n\nfeature_X_train, feature_X_valid, feature_y_train, feature_y_test = train_test_split(train_data_copy, y, random_state=0)\n\nfeature_classifier.fit(feature_X_train, feature_y_train, feature_model__verbose=False)\nfeature_predictions = feature_classifier.predict(feature_X_valid)\n\n# Implementing our earlier custom defined function for RMSLE\n# Root Mean Squared Log Error . This metric is used when the Target variable is converted into Log(Target).\n#print('RMSLE from custom-defined function', root_mean_squared_log_error(feature_y_test, feature_predictions))\n\n# Implementing Scikit-learn function for RMSLE\nprint('RMSLE using scikit-learn function', np.sqrt(mean_squared_log_error(feature_y_test, feature_predictions)))\nprint('Root Mean Square Error is ', np.sqrt(mean_squared_error(feature_y_test, feature_predictions)))\n","metadata":{"execution":{"iopub.status.busy":"2022-06-16T13:56:35.274521Z","iopub.execute_input":"2022-06-16T13:56:35.275602Z","iopub.status.idle":"2022-06-16T13:56:42.242756Z","shell.execute_reply.started":"2022-06-16T13:56:35.275566Z","shell.execute_reply":"2022-06-16T13:56:42.241466Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"ridge = Ridge(alpha=0.5)\nfeature_classifier = Pipeline(steps=[('feature_preprocessor', feature_preprocessor),\n                      ('feature_model',ridge)\n                     ])\n\nfeature_X_train, feature_X_valid, feature_y_train, feature_y_test = train_test_split(train_data_copy, y, random_state=0)\n\nfeature_classifier.fit(feature_X_train, feature_y_train)\nfeature_predictions = feature_classifier.predict(feature_X_valid)\n\nprint('Root Mean Square Error is ', np.sqrt(mean_squared_error(feature_y_test, feature_predictions)))\n\n\n# Implementing Scikit-learn function for RMSLE\nprint('RMSLE using scikit-learn function', np.sqrt(mean_squared_log_error(feature_y_test, feature_predictions)))","metadata":{"execution":{"iopub.status.busy":"2022-06-16T13:58:16.319898Z","iopub.execute_input":"2022-06-16T13:58:16.320920Z","iopub.status.idle":"2022-06-16T13:58:16.408597Z","shell.execute_reply.started":"2022-06-16T13:58:16.320882Z","shell.execute_reply":"2022-06-16T13:58:16.407462Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"lasso = Lasso(alpha=0.5)\nfeature_classifier = Pipeline(steps=[('feature_preprocessor', feature_preprocessor),\n                      ('feature_model',lasso)\n                     ])\n\nfeature_X_train, feature_X_valid, feature_y_train, feature_y_test = train_test_split(train_data_copy, y, random_state=0)\n\nfeature_classifier.fit(feature_X_train, feature_y_train)\nfeature_predictions = feature_classifier.predict(feature_X_valid)\n\nprint('Root Mean Square Error is ', np.sqrt(mean_squared_error(feature_y_test, feature_predictions)))\n\n\n# Implementing Scikit-learn function for RMSLE\nprint('RMSLE using scikit-learn function', np.sqrt(mean_squared_log_error(feature_y_test, feature_predictions)))","metadata":{"execution":{"iopub.status.busy":"2022-06-16T13:59:24.798241Z","iopub.execute_input":"2022-06-16T13:59:24.798708Z","iopub.status.idle":"2022-06-16T13:59:25.855396Z","shell.execute_reply.started":"2022-06-16T13:59:24.798665Z","shell.execute_reply":"2022-06-16T13:59:25.853854Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"\nfeature_classifier = Pipeline(steps=[('feature_preprocessor', feature_preprocessor),\n                      ('feature_model',RandomForestRegressor(max_depth=50, random_state=2))\n                     ])\n\nfeature_X_train, feature_X_valid, feature_y_train, feature_y_test = train_test_split(train_data_copy, y, random_state=0)\n\nfeature_classifier.fit(feature_X_train, feature_y_train)\nfeature_predictions = feature_classifier.predict(feature_X_valid)\n\nprint('Root Mean Square Error is ', np.sqrt(mean_squared_error(feature_y_test, feature_predictions)))\n\n\n# Implementing Scikit-learn function for RMSLE\nprint('RMSLE using scikit-learn function', np.sqrt(mean_squared_log_error(feature_y_test, feature_predictions)))","metadata":{"execution":{"iopub.status.busy":"2022-06-16T14:01:45.323823Z","iopub.execute_input":"2022-06-16T14:01:45.324278Z","iopub.status.idle":"2022-06-16T14:01:53.266131Z","shell.execute_reply.started":"2022-06-16T14:01:45.324242Z","shell.execute_reply":"2022-06-16T14:01:53.264984Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"X_test_original = pd.read_csv('../input/house-prices-advanced-regression-techniques/test.csv')\n\nX_test_original['years_since_update'] = X_test_original['YearRemodAdd'] - X_test_original['YearBuilt']\nX_test_original['garage_value'] = X_test_original['YearBuilt'] * X_test_original['GarageCars']\n\n#X_test_original = X_test_original.drop(columns=['GarageCars'])\n\nfeature_classifier.fit(train_data_copy, y)\n\nmodel_predictions = feature_classifier.predict(test_data)\noutput = pd.DataFrame({'Id': X_test_original.Id,\n                       'SalePrice': model_predictions})\noutput.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-06-16T14:02:25.058390Z","iopub.execute_input":"2022-06-16T14:02:25.059231Z","iopub.status.idle":"2022-06-16T14:02:37.179606Z","shell.execute_reply.started":"2022-06-16T14:02:25.059194Z","shell.execute_reply":"2022-06-16T14:02:37.178713Z"},"trusted":true},"execution_count":44,"outputs":[]}]}